# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
etcd
nodes
loadbalancers

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=cloud-user
ansible_become=yes

debug_level=2

openshift_deployment_type=openshift-enterprise

os_sdn_network_plugin_name=redhat/openshift-ovs-networkpolicy

# sets up the correct auth provider - openid integrated SSO or htpasswd as fallback
{% if sso_client_id is defined and sso_client_id != "<client ID>"  %}
openshift_master_identity_providers=[{"name": "UKCloud-SSO", "login": "true", "challenge": "false", "kind": "OpenIDIdentityProvider", "client_id": "{{ sso_client_id }}", "client_secret": "{{ sso_client_secret }}", "claims": {"id": ["sub"], "preferredUsername": ["preferred_username"], "name": ["name"], "email": ["email"]}, "urls": {{ (sso_urls|from_json)|json_query('{authorize: authorize, token: token, userInfo: userInfo}')|to_json }} }]
openshift_master_openid_ca_file="/etc/ssl/certs/ca-bundle.crt"
openshift_master_logout_url='{{ (sso_urls|from_json).logout }}'
{% else %}
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_users={'admin': '{{ admin_password }}'}
{% endif %}

openshift_master_external_ip_network_cidrs=['{{ external_service_subnet }}']

openshift_master_cluster_method=native
openshift_master_cluster_hostname=console.{{ localDomainSuffix }}
openshift_master_cluster_public_hostname=ocp.{{ domainSuffix }}

openshift_release=v{{ openshiftVersion }}
### review how this is working at the moment - no sign of revisions of minor releases in rh container registry
openshift_image_tag=v{{ installed_version[0] }}
# openshift_image_tag=v{{ openshiftVersion }}

### deprecated - probably can be removed ###
#openshift_set_hostname=true
# Makes the openshift_ip setting in the host groups are below take effect.
# This sets the nodeIP setting in node-config on each host to the correct IP
#openshift_set_node_ip=true

openshift_master_default_subdomain={{ domainSuffix }}

# OpenShift project template edits
openshift_project_request_template_manage=True
osm_project_request_template=default/project-request
openshift_project_request_template_name=project-request
openshift_project_request_template_namespace=default
openshift_project_request_template_edits=[{"key": "objects","action": "append","value": {"kind": "NetworkPolicy","apiVersion": "networking.k8s.io/v1","metadata": {"name": "allow-from-same-and-privileged-namespaces"},"spec": {"ingress": [{"from": [{"podSelector": {}}]},{"from": [{"namespaceSelector": {"matchLabels": {"state": "privileged"}}}]}]}}}]



# Set Docker log driver to write to journald
openshift_docker_options="--log-driver=json-file --log-opt max-size=3M --log-opt max-file=5"

# ROUTER
openshift_hosted_router_selector='router=true'

# REGISTRY
openshift_hosted_registry_selector='node-role.kubernetes.io/infra=true'
openshift_hosted_registry_replicas=1

# Any S3 service (Minio, ExoScale, ...): Basically the same as above
# but with regionendpoint configured
# S3 bucket must already exist.
openshift_hosted_registry_storage_kind=object
openshift_hosted_registry_storage_provider=s3
openshift_hosted_registry_storage_s3_accesskey={{ s3accesskey }}
openshift_hosted_registry_storage_s3_secretkey={{ s3secretkey }}
openshift_hosted_registry_storage_s3_regionendpoint={{ s3regionendpoint }}
openshift_hosted_registry_storage_s3_bucket={{ s3bucketname }}
openshift_hosted_registry_storage_s3_region=bucket_region
openshift_hosted_registry_storage_s3_chunksize=26214400
openshift_hosted_registry_storage_s3_rootdirectory=/registry
openshift_hosted_registry_pullthrough=true
openshift_hosted_registry_acceptschema2=true
openshift_hosted_registry_enforcequota=true

### sets the default node selector projects will use on pod scheduling
osm_default_node_selector='tenant=true'

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

# openstack cinder integration for persistant volume claims
openshift_cloudprovider_kind=openstack
openshift_cloudprovider_openstack_auth_url={{ osAuthUrl }}
openshift_cloudprovider_openstack_username={{ openstackOpenshiftUsername}}
openshift_cloudprovider_openstack_password={{ openstackOpenshiftPassword }}
openshift_cloudprovider_openstack_tenant_id={{ osTenantId }}
openshift_cloudprovider_openstack_tenant_name={{ osTenantName }}
openshift_cloudprovider_openstack_region={{ osRegion }}
openshift_cloudprovider_openstack_domain_name={{ osDomainID }}
openshift_cloudprovider_openstack_blockstorage_ignore_volume_az="yes"

# openstack default storage class configuration since this gets created for us regardless now!
openshift_storageclass_default= "true"
openshift_storageclass_name= "tier2"
openshift_storageclass_provisioner= "cinder"
openshift_storageclass_parameters={ 'type': 'TIER2', 'availability': 'nova' }

# openshift stats deployment - comment out if no stats deployment is required. this will run on the worker nodes
openshift_metrics_install_metrics=true
openshift_metrics_hawkular_hostname=hawkular-metrics.{{ domainSuffix }}
openshift_metrics_cassandra_storage_type=dynamic
openshift_metrics_cassandra_pvc_size=10Gi
# Node selectors for hawkular metrics
openshift_metrics_cassandra_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_metrics_heapster_nodeselector={"node-role.kubernetes.io/infra":"true"}

# openshift aggregated logging deployment - comment out if no aggregated logging  deployment is required. this will run on the worker nodes
openshift_logging_install_logging={{ installLogging  }}
openshift_logging_purge_logging=false
openshift_logging_storage_kind=dynamic
openshift_logging_kibana_hostname=kibana.{{ domainSuffix }}
# Value below was trying to call openshift_master_cluster_public_hostname but unable to as set in this file. Changed to ocp.{{ domainSuffix }}
openshift_logging_master_public_url=https://ocp.{{ domainSuffix }}:8443
# Value below was trying to call openshift_master_cluster_hostname but unable to as set in this file. Changed to console.{{ localDomainSuffix }}
openshift_logging_master_url=https://console.{{ localDomainSuffix }}:8443
openshift_logging_use_journal=true
openshift_logging_use_ops=false
openshift_logging_es_cluster_size={{ loggingClusterSize }}
openshift_logging_es_pvc_size=40Gi
# Node selectors for aggregated logging
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true"}
# Set the default memory limit for elasticsearch so that the pod can be scheduled to an infra node
openshift_logging_es_memory_limit="8Gi"
openshift_logging_curator_default_days=14

# fix to make the ansible service broker deploy to the correct nodes rather than all or none
# RH ticket #02020379
template_service_broker_selector={"node-role.kubernetes.io/infra":"true"}

# Ansible service broker node selector
ansible_service_broker_node_selector={"node-role.kubernetes.io/infra":"true"}

# New v3.11 features
# Monitoring
openshift_cluster_monitoring_operator_install=true
openshift_cluster_monitoring_operator_prometheus_storage_enabled=true
openshift_cluster_monitoring_operator_prometheus_storage_class_name=tier2
openshift_cluster_monitoring_operator_prometheus_storage_capacity="50Gi"
openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true
openshift_cluster_monitoring_operator_alertmanager_storage_class_name=tier2
openshift_cluster_monitoring_operator_alertmanager_storage_capacity="2Gi"
#openshift_prometheus_storage_type=pvc
#openshift_prometheus_alertmanager_storage_type=pvc
#openshift_prometheus_alertbuffer_storage_type=pvc

# Set the v3.11+ console hostname
openshift_console_hostname=console.{{ domainSuffix }}

{% if ocpBranding is defined and ocpBranding and ocpBrandingUrl is defined %}
# Brand the ocp web console
openshift_web_console_extension_stylesheet_urls=['{{ ocp_branding_url }}']
{% endif %}

# New registry authentication options
oreg_auth_url={{ registryUrl }}
oreg_auth_user={{ registryUser }}
oreg_auth_password={{ registryPassword }}

{% if multinetwork %}
# Proxy variables for use when net2 deployment is in place
openshift_http_proxy=http://{{ haproxy_vip }}:3128
openshift_https_proxy=http://{{ haproxy_vip }}:3128
openshift_no_proxy='.{{ localDomainSuffix }},frn00006.cni.ukcloud.com,cor00005.cni.ukcloud.com,169.254.169.254'
#openshift_generate_no_proxy_hosts=true
#openshift_builddefaults_git_http_proxy=http://{{ haproxy_vip }}:3128
#openshift_builddefaults_git_https_proxy=http://{{ haproxy_vip }}:3128
#openshift_builddefaults_http_proxy=http://{{ haproxy_vip }}:3128
#openshift_builddefaults_https_proxy=http://{{ haproxy_vip }}:3128
{% endif %}

### Setup the default node labels for each node type
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true'], 'edits': [{ 'key': 'kubeletArguments.kube-reserved','value': ["cpu=200m,memory=512Mi"]}]},{'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true','infra=true','router=true'{% if extra_gateway_vip is defined %},'router-private=true'{% endif %}], 'edits': [{ 'key': 'kubeletArguments.kube-reserved','value': ["cpu=200m,memory=512Mi"]}]},{'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true','internet=true','tenant=true'], 'edits': [{ 'key': 'kubeletArguments.kube-reserved','value': ["cpu=200m,memory=512Mi"]}]},{'name': 'node-config-compute-net2', 'labels': ['node-role.kubernetes.io/compute=true','router=secondary','net2=true','tenant=true'], 'edits': [{ 'key': 'kubeletArguments.kube-reserved','value': ["cpu=200m,memory=512Mi"]}]}]

# Create an OSEv3 group that contains the masters and nodes groups
# host group for masters
[masters]
{% for ip, hostname in master_details.items() %}
{{ hostname }}
{% endfor %}

[etcd]
{% for ip, hostname in master_details.items() %}
{{ hostname }}
{% endfor %}

[loadbalancers:children]
loadbalancers_controlplane
loadbalancers_dataplane

[loadbalancers_dataplane:children]
loadbalancers_internet_dataplane
{% if multinetwork %}
loadbalancers_net2_dataplane
{% endif %}

[loadbalancers_controlplane]
{% for ip, hostname in control_plane_details.items() %}
{{ hostname }}
{% endfor %}

[loadbalancers_internet_dataplane]
{% for ip, hostname in data_plane_internet.items() %}
{{ hostname }}
{% endfor %}

[loadbalancers_net2_dataplane]
{% if multinetwork %}
{% for ip, hostname in data_plane_net2.items() %}
{{ hostname }}
{% endfor %}
{% endif %}

[dns]
{% for ip, hostname in control_plane_details.items() %}
{{ hostname }}
{% endfor %}

[dns_net2:children]
{% if multinetwork %}
loadbalancers_net2_dataplane
{% endif %}

# host group for nodes, includes region info
# Routers are placed only on first 3 workers
[nodes:children]
nodes_internet
nodes_infra
{% if multinetwork %}
nodes_net2
{% endif %}

[nodes_internet]
{% for ip, hostname in master_details.items() %}
{{ hostname }} openshift_node_group_name='node-config-master'
{% endfor %}
{% if worker_details_internet != None %}
{% for ip, hostname in worker_details_internet.items() %}
{{ hostname }} openshift_node_group_name='node-config-compute'
{% endfor %}
{% endif %}

[nodes_infra]
{% for ip, hostname in infrastructure_node_details.items() %}
{{ hostname }} openshift_node_group_name='node-config-infra'
{% endfor %}

[nodes_net2]
{% if multinetwork %}
{% for ip, hostname in worker_details_net2.items() %}
{{ hostname }} openshift_node_group_name='node-config-compute-net2'
{% endfor %}
{% endif %}

# groups to capture all of the hosts with connectivity to each network connection
[internet_all:children]
loadbalancers_controlplane
loadbalancers_internet_dataplane
nodes_internet
nodes_infra

[net2_all:children]
loadbalancers_net2_dataplane
nodes_net2
